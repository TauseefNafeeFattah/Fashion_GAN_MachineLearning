{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP7zMWqpOPbN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_FoND9VRSJD"
      },
      "outputs": [],
      "source": [
        "# maintain consistent performance\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "# check if GPU is available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12JimsBlPNBw"
      },
      "outputs": [],
      "source": [
        "# load the Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koohyY3JPT_9"
      },
      "outputs": [],
      "source": [
        "# merge the training and testing sets and normalize the images from [0,255] to [0,1]\n",
        "dataset = np.concatenate([x_train, x_test], axis=0)\n",
        "\n",
        "dataset = np.expand_dims(dataset, -1).astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYhwHE5vPXhH"
      },
      "outputs": [],
      "source": [
        "# create a tensorflow dataset object\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = np.reshape(dataset, (-1, 28, 28, 1))\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQtRFmjQQDdk"
      },
      "outputs": [],
      "source": [
        "NOISE_DIM = 150\n",
        "\n",
        "generator = keras.models.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(NOISE_DIM,)),\n",
        "  layers.Dense(7*7*256),\n",
        "  layers.Reshape(target_shape=(7, 7, 256)),\n",
        "  layers.Conv2DTranspose(256, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\"),\n",
        "  layers.Conv2DTranspose(128, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\"),\n",
        "  layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")\n",
        "])\n",
        "\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-uoPaAMQDR5"
      },
      "outputs": [],
      "source": [
        "discriminator = keras.models.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "  layers.Conv2D(256, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
        "  layers.Conv2D(128, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(64, activation=\"relu\"),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNzIEW9AQC_X"
      },
      "outputs": [],
      "source": [
        "# setting learning rates\n",
        "optimizerG = keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.5)\n",
        "optimizerD = keras.optimizers.Adam(learning_rate=0.00003, beta_1=0.5)\n",
        "\n",
        "# binary classifier (real or fake)\n",
        "lossFn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# accuracy metric\n",
        "gAccMetric = tf.keras.metrics.BinaryAccuracy()\n",
        "dAccMetric = tf.keras.metrics.BinaryAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yds_C9jvQtXk"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def trainDStep(data):\n",
        "\n",
        "  batchSize = tf.shape(data)[0]\n",
        "\n",
        "  noise = tf.random.normal(shape=(batchSize, NOISE_DIM))\n",
        "\n",
        "  # concatenate the real and fake labels\n",
        "  y_true = tf.concat(\n",
        "    [\n",
        "      # the original data is real, labeled with 1\n",
        "      tf.ones(batchSize, 1),\n",
        "      # the forged data is fake, labeled with 0\n",
        "      tf.zeros(batchSize, 1)\n",
        "    ],\n",
        "    axis=0\n",
        "  )\n",
        "\n",
        "  # record the calculated gradients\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    fake = generator(noise)\n",
        "\n",
        "    x = tf.concat([data, fake], axis=0)\n",
        "\n",
        "    y_pred = discriminator(x)\n",
        "\n",
        "    discriminatorLoss = lossFn(y_true, y_pred)\n",
        "\n",
        "  # apply the backward path and update weights\n",
        "  grads = tape.gradient(discriminatorLoss, discriminator.trainable_weights)\n",
        "  optimizerD.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
        "\n",
        "  # report accuracy\n",
        "  dAccMetric.update_state(y_true, y_pred)\n",
        "\n",
        "  # return the loss for visualization\n",
        "  return {\n",
        "      \"discriminator_loss\": discriminatorLoss,\n",
        "      \"discriminator_accuracy\": dAccMetric.result()\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtaYpJw4QtM1"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def trainGStep(data):\n",
        "  batchSize = tf.shape(data)[0]\n",
        "  noise = tf.random.normal(shape=(batchSize, NOISE_DIM))\n",
        "\n",
        "  y_true = tf.ones(batchSize, 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = discriminator(generator(noise))\n",
        "    generatorLoss = lossFn(y_true, y_pred)\n",
        "\n",
        "  grads = tape.gradient(generatorLoss, generator.trainable_weights)\n",
        "  optimizerG.apply_gradients(zip(grads, generator.trainable_weights))\n",
        "\n",
        "  gAccMetric.update_state(y_true, y_pred)\n",
        "\n",
        "  return {\n",
        "      \"generator_loss\": generatorLoss,\n",
        "      \"generator_accuracy\": gAccMetric.result()\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtjsJ7UlRL2e"
      },
      "outputs": [],
      "source": [
        "def plotImages(model):\n",
        "    # plots the images\n",
        "    images = model(np.random.normal(size=(81, NOISE_DIM)))\n",
        "\n",
        "    plt.figure(figsize=(9, 9))\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "        plt.subplot(9,9,i+1)\n",
        "        plt.imshow(np.squeeze(image, -1), cmap=\"Greys_r\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RwFGgNQROgm"
      },
      "outputs": [],
      "source": [
        "for epoch in range(30):\n",
        "\n",
        "  # accumulate the loss to calculate the average at the end of the epoch\n",
        "  dLossSum = 0\n",
        "  gLossSum = 0\n",
        "  dAccSum = 0\n",
        "  gAccSum = 0\n",
        "  cnt = 0\n",
        "\n",
        "  # loop the dataset one batch at a time\n",
        "  for batch in dataset:\n",
        "\n",
        "    # train the discriminator\n",
        "    dLoss = trainDStep(batch)\n",
        "    dLossSum += dLoss['discriminator_loss']\n",
        "    dAccSum += dLoss['discriminator_accuracy']\n",
        "\n",
        "    # train the generator\n",
        "    gLoss = trainGStep(batch)\n",
        "    gLossSum += gLoss['generator_loss']\n",
        "    gAccSum += gLoss['generator_accuracy']\n",
        "\n",
        "    cnt += 1\n",
        "\n",
        "  # log the performance\n",
        "  print(\"E:{}, Loss G:{:0.4f}, Loss D:{:0.4f}, Acc G:%{:0.2f}, Acc D:%{:0.2f}\".format(\n",
        "      epoch,\n",
        "      gLossSum/cnt,\n",
        "      dLossSum/cnt,\n",
        "      100 * gAccSum/cnt,\n",
        "      100 * dAccSum/cnt\n",
        "  ))\n",
        "    \n",
        "  if epoch % 2 == 0:\n",
        "    plotImages(generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3heiT-WROWp"
      },
      "outputs": [],
      "source": [
        "# check a generated sample\n",
        "images = generator(np.random.normal(size=(81, NOISE_DIM)))\n",
        "\n",
        "plt.figure(figsize=(9, 9))\n",
        "\n",
        "for i, image in enumerate(images):\n",
        "    plt.subplot(9,9,i+1)\n",
        "    plt.imshow(np.squeeze(image, -1), cmap=\"Greys_r\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show();"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}